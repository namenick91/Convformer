\section{Заключение}

% Предложенная комбинация ConvStem + FAVOR+ + Autoformer-подобная декомпозиция демонстрирует 
% улучшение качества на длинных горизонтах при сопоставимых вычислительных затратах 
% на рассматриваемых настройках. Абляционные исследования подтверждают вклад 
% модулей на большинстве горизонтов: ConvStem -- выделение локальных мотивов; FAVOR+ -- 
% масштабируемые дальние зависимости; декомпозиция -- устойчивость к тренду/сезонности. 
% На ETT наибольшие выигрыши достигаются при длительных горизонтах (336-720), тогда как 
% на средних горизонтах Autoformer остается сопоставимым. 

% При этом исследование имеет ряд ограничений: 
% % в качестве бенчмарка был рассмотрен один набор данных (ETT), 
% cross-attention сохраняет квадратичную сложность, 
% % чувствительность MAPE/MSPE к масштабам признаков, 
% отсутствие оценки неопределённости. 
% Перспективными направлениями являются
% линеаризация cross-attention, адаптивный выбор ранга $r$ в FAVOR+, 
% % расширение набора датасетов, 
% включение калибровки и методов, устойчивых к дрейфу распределений.

В работе предложен Convformer -- модульная модификация Informer, объединяющая сверточный входной
блок ConvStem, линейное внимание FAVOR+ и Autoformer-подобную декомпозицию временного
ряда. Такая комбинация индуктивных смещений позволяет более явно разделить задачи
выделения локальных паттернов, моделирования дальних зависимостей и компенсации
нестационарности.

На семи стандартных бенчмарках LSTF (ETTh1/ETTh2, ECL, Exchange, Illness, Traffic,
Weather) Convformer обеспечивает снижение ошибок прогноза на большинстве горизонтов по сравнению
с Informer и Performer при сопоставимых вычислительных затратах. Относительно
Autoformer выигрыш достигается на большинстве датасетов, за исключением Exchange и
отдельных длинных горизонтов, где Autoformer остаётся конкурентным. Абляционные
исследования показывают, что каждый из трёх модулей вносит вклад в качество: ConvStem
улучшает обработку краткосрочных мотивов и устойчивость к локальным скачкам, FAVOR+
даёт масштабируемый учёт глобальных связей на длинных горизонтах, декомпозиция
повышает устойчивость к тренду и мультисезонности. На ETT наибольшие выигрыши
достигаются при длительных горизонтах (336–720), тогда как на средних горизонтах
Autoformer остаётся сопоставимым. Абляционные исследования и подробный бенчмарк приведены
в разделах~\ref{sec:experiments}, \ref{sec:ablations} 
и~\hyperref[sec:appendixA]{приложении A}. 

При этом исследование имеет ряд ограничений: cross-attention сохраняет
квадратичную сложность; настройки модели и гиперпараметров подбирались в ограниченном
классе конфигураций; не проводилось систематическое исследование глубины, ширины и
размеров карт случайных признаков; отсутствие оценки неопределённости.
Перспективными направлениями являются линеаризация cross-attention
и/или внедрение модуля дистилляции в декодер, адаптивный выбор
ранга $r$ в FAVOR+, а также включение калибровки и методов, устойчивых к дрейфу 
распределения данных. 
Отдельная линия работы -- перенос предложенной модульной схемы
на другие архитектуры блоков энкодера/декодера и задачи за пределами LSTF.
