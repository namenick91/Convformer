\section{Методология}

\textbf{Повышение эффективности извлечения локальных паттернов} $\quad$
Для усиления способности модели к распознаванию краткосрочных закономерностей мы 
предлагаем заменить стандартный механизм представления значений в блоке с обработкой 
входных данных в Informer~\cite{informer}
на сверточный блок ConvStem, при этом оставив позиционное с темпоральным эмбеддированием. 
Данный блок сочетает проекцию входных признаков через 
точечную свёртку с последующими операциями широкой и глубинной свёрток, дополненных 
нормализацией и нелинейностью. Такое построение позволяет модели фиксировать повторяющиеся 
локальные мотивы и вариации формы сигналов непосредственно на этапе встраивания, 
ещё до применения механизмов внимания. В результате глобальное внимание может быть 
сфокусировано преимущественно на долговременных зависимостях, тогда как локальная 
динамика эффективно извлекается специализированным сверточным модулем.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{main/ConvStem.pdf}
    \caption{Схема блока ConvStem. 
    Здесь: $\mathcal{X}_t \in \mathbb{R}^{L_x \times C_{\text{in}}}, 
    \mathcal{X}'_t \in \mathbb{R}^{L_x \times d}$, где $d$-размерность модели;
    \textrm{IN}-нормализация по экземплярам (Instance Normalization~\cite{InstanceNorm}). 
    Псевдокод приведен в \ref{alg:convstem_pseudocode}.} 
    \label{fig:ConvStemScheme}
\end{figure}

\textbf{Масштабируемое моделирование глобальных зависимостей} $\quad$
В качестве механизма внимания предлагается использовать FAVOR+
(Performer~\cite{performer}) вместо ProbSparse (оригинально применяемого 
в Informer~\cite{informer}).
Подобно тому, как в оригинальном Informer~\cite{informer} 
механизм FullAttention~\cite{transformer} был
заменен на ProbSparse исключительно для вычисления self-attention (механизм самовнимания) в 
слоях кодировщика и декодера, в нашей модификации ProbSparse заменяется 
на FAVOR+ в тех же местах, тогда как cross-attention останется 
реализованным через полное внимание (FullAttention).
Данную замену мы мотивируем тем, что FAVOR+ обеспечивает  
несмещённую аппроксимацию softmax-ядра с линейной по времени
и памяти сложностью~\cite{performer}, что гарантирует предсказуемую и масштабируемую
работу на очень длинных последовательностях в условиях ограниченных
ресурсов GPU. 
При этом точность аппроксимации управляется числом случайных признаков $r$: 
при меньших значениях достигается высокая скорость, при больших - более 
точное восстановление распределения внимания. Такая настраиваемость делает механизм 
универсальным инструментом, позволяющим варьировать баланс между эффективностью и 
качеством.
Кроме того, отказ от ручной выборки top-$u$ запросов
позволяет модели учитывать любые глобальные зависимости, а не только
наиболее сильные периодические, что критично для учёта 
непериодичных скачков в реальных временных рядах.

\textbf{Явное разделение тренда и сезонности} $\quad$ Чтобы явно разделить тренд и сезонную компоненту, 
мы интегрировали механизм декомпозиции временных рядов, заимствованный из архитектуры 
Autoformer~\cite{autoformer}. После каждого блока self-attention
скрытое представление разлагается на два канала: низкочастотный тренд и остаточную 
компоненту, содержащую более стационарные сезонные и случайные колебания. Тренд передаётся по 
остаточному пути, обеспечивая стабильность прогнозов при сдвигах уровня, тогда как внимание 
применяется к сезонной компоненте, где наиболее выражены периодические и локальные закономерности. 
Такая стратегия снижает вариативность обучения, повышает способность к экстраполяции за пределами 
обучающего диапазона и улучшает интерпретируемость результатов за счёт явного выделения трендовой 
и сезонной составляющих.

\rule{\textwidth}{0.6pt}
Схема механизма декомпозиции ряда~\cite{autoformer}\\[-5pt] {\color{red} УБРАТЬ/ИЗМЕНИТЬ}
\rule{\textwidth}{0.4pt}

\begin{center}
    \begin{minipage}{0.55\linewidth}
        \noindent Input: $\text{X}$ \\
        
        \noindent (1) [SeriesDecomp] $\rightarrow$ $\text{Trend}$, $\text{Seasonal}$ \\
        
        \qquad where \vspace{-30pt} \begin{flalign*}
            & \hspace{70pt} \text{Trend} = \text{AvgPool}(\text{Padding}(\text{X})) && \\ 
            & \hspace{70pt} \text{Seasonal} = \text{X} - \text{Trend} 
        \end{flalign*}

        \noindent (2) [self-attention mechanism] on $\text{Seasonal}$

        \qquad $\rightarrow$ $\text{Seasonal}'$ (+ residual connection, normalization, etc.) \\ 

        \noindent (3) [Feed-Forward] on $\text{Seasonal}' \rightarrow \widetilde{\text{Seasonal}}$ \\
        
        \noindent (4) Re-compose $\text{X} = \widetilde{\text{Seasonal}} + \text{Trend}$
    \end{minipage}
\end{center}

\rule{\textwidth}{0.4pt}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{main/Convformer.pdf}
    \caption{Архитектура Convformer. Входные данные (для декодера -- их сезонная составляющая) 
    проходят через блок ConvEmbedding ({\color{DDD0F6} фиолетовый} блок), состоящий из 
    ConvStem, PositionalEmbedding и TemporalEmbedding (подробнее см. 
    % в разделе~\ref{sec:appendixD}
    {\color{red} ДОБАВИТЬ ССЫЛКУ НА АППЕНДИКС}
    ).
    Энкодер устраняет долгосрочную трендово-циклическую составляющую с помощью блоков декомпозиции рядов 
    ({\color{BDE5F6} синие} блоки) и сосредотачивается на моделировании сезонных компонент.
    Блок дистилляции $N^*$ в энкодере ({\color{F6F2AE} жёлтый} блок) включён во все слои, кроме последнего.
    Декодер постепенно накапливает трендовую составляющую, извлекаемую из скрытых переменных.
    Прошлая сезонная информация, полученная от энкодера, используется в классическом (полном) механизме внимания 
    (центральный {\color{C5E6AE} зелёный} блок в декодере).}
    \label{fig:ConvformerScheme}
\end{figure}

В итоге архитектура объединяет сильные стороны
трёх подходов: локальное кодирование паттернов через ConvStem, линейное глобальное внимание 
FAVOR+ и явную декомпозицию ряда из Autoformer~\cite{autoformer}, 
сохраняя при этом полную совместимость с остальными гиперпараметрами 
оригинального Informer~\cite{informer}.

% \begin{center}
%     \begin{minipage}{0.55\linewidth}
%         \noindent Input: $\text{H}^{(l-1)}$ \\
        
%         \noindent (1) [SeriesDecomp] $\rightarrow$ $\text{Trend}^{(l)}$, $\text{Seasonal}^{(l)}$ \\
        
%         \qquad where \vspace{-30pt} \begin{flalign*}
%             & \hspace{70pt} \text{Trend}^{(l)} = \text{SMA}(\text{H}^{(l-1)}) && \\ 
%             & \hspace{70pt} \text{Seasonal}^{(l)} = \text{H}^{(l-1)} - \text{Trend}^{(l)} 
%         \end{flalign*}

%         \noindent (2) [selft-attention mechanism] on $\text{Seasonal}^{(l)}$

%         \qquad $\rightarrow$ $\text{Seasonal}'^{(l)}$ (+ residual connection, normalization, etc.) \\ 

%         \noindent (3) [Feed-Forward] on $\text{Seasonal}'^{(l)} \rightarrow \widetilde{\text{Seasonal}}^{(l)}$ \\
        
%         \noindent (4) Re-compose $\text{H}^{(l)} = \widetilde{\text{Seasonal}}^{(l)} + \text{Trend}^{(l)}$
%     \end{minipage}
% \end{center}
