\section{Методология}

\textbf{Повышение эффективности извлечения локальных паттернов} $\quad$
Для усиления способности модели к распознаванию краткосрочных закономерностей мы 
предлагаем заменить стандартный механизм представления значений (TokenEmbedding) 
на сверточный блок ConvStem. Данный блок сочетает проекцию входных признаков через 
точечную свёртку с последующими операциями широкой и глубинной свёрток, дополненных 
нормализацией и нелинейностью. Такое построение позволяет модели фиксировать повторяющиеся 
локальные мотивы и вариации формы сигналов непосредственно на этапе встраивания, 
ещё до применения механизмов внимания. В результате глобальное внимание может быть 
сфокусировано преимущественно на долговременных зависимостях, тогда как локальная 
динамика эффективно извлекается специализированным сверточным модулем.

\textbf{Масштабируемое моделирование глобальных зависимостей} $\quad$
В качестве механизма внимания предлагается использовать FAVOR+
(Performer~\cite{performer}) вместо ProbSparse (оригинально применяемого 
в Informer~\cite{informer}).
Подобно тому, как в оригинальном Informer~\cite{informer} 
механизм FullAttention~\cite{transformer} был
заменен на ProbSparse исключительно для вычисления self-attention (механизм самовнимания) в 
слоях кодировщика и декодера, в нашей модификации ProbSparse заменяется 
на FAVOR+ в тех же местах, тогда как cross-attention останется 
реализованным через полное внимание (FullAttention).
Данную замену мы мотивируем тем, что FAVOR+ обеспечивает  
несмещённую аппроксимацию softmax-ядра с линейной по времени
и памяти сложностью~\cite{performer}, что гарантирует предсказуемую и масштабируемую
работу на очень длинных последовательностях в условиях ограниченных
ресурсов GPU. 
При этом точность аппроксимации управляется числом случайных признаков $r$: 
при меньших значениях достигается высокая скорость, при больших - более 
точное восстановление распределения внимания. Такая настраиваемость делает механизм 
универсальным инструментом, позволяющим варьировать баланс между эффективностью и 
качеством.
Кроме того, отказ от ручной выборки top-$u$ запросов
позволяет модели учитывать любые глобальные зависимости, а не только
наиболее сильные периодические, что критично для учёта 
непериодичных скачков в реальных временных рядах.

\textbf{Явное разделение тренда и сезонности} $\quad$ Чтобы явно разделить тренд и сезонную компоненту, 
мы интегрировали механизм декомпозиции временных рядов, заимствованный из архитектуры 
Autoformer~\cite{autoformer}. После каждого блока self-attention
скрытое представление разлагается на два канала: низкочастотный тренд и остаточную 
компоненту, содержащую более стационарные сезонные и случайные колебания. Тренд передаётся по 
остаточному пути, обеспечивая стабильность прогнозов при сдвигах уровня, тогда как внимание 
применяется к сезонной компоненте, где наиболее выражены периодические и локальные закономерности. 
Такая стратегия снижает вариативность обучения, повышает способность к экстраполяции за пределами 
обучающего диапазона и улучшает интерпретируемость результатов за счёт явного выделения трендовой 
и сезонной составляющих.

\rule{\textwidth}{0.6pt}
Схема механизма декомпозиции ряда~\cite{autoformer}\\[-5pt]
\rule{\textwidth}{0.4pt}

\begin{center}
    \begin{minipage}{0.55\linewidth}
        \noindent Input: $\text{X}$ \\
        
        \noindent (1) [SeriesDecomp] $\rightarrow$ $\text{Trend}$, $\text{Seasonal}$ \\
        
        \qquad where \vspace{-30pt} \begin{flalign*}
            & \hspace{70pt} \text{Trend} = \text{AvgPool}(\text{Padding}(\text{X})) && \\ 
            & \hspace{70pt} \text{Seasonal} = \text{X} - \text{Trend} 
        \end{flalign*}

        \noindent (2) [self-attention mechanism] on $\text{Seasonal}$

        \qquad $\rightarrow$ $\text{Seasonal}'$ (+ residual connection, normalization, etc.) \\ 

        \noindent (3) [Feed-Forward] on $\text{Seasonal}' \rightarrow \widetilde{\text{Seasonal}}$ \\
        
        \noindent (4) Re-compose $\text{X} = \widetilde{\text{Seasonal}} + \text{Trend}$
    \end{minipage}
\end{center}

\rule{\textwidth}{0.4pt}

В итоге архитектура объединяет сильные стороны
трёх подходов: локальное кодирование паттернов через ConvStem, линейное глобальное внимание 
FAVOR+ и явную декомпозицию ряда из Autoformer~\cite{autoformer}, 
сохраняя при этом полную совместимость с остальными гиперпараметрами 
оригинального Informer~\cite{informer}.

% \begin{center}
%     \begin{minipage}{0.55\linewidth}
%         \noindent Input: $\text{H}^{(l-1)}$ \\
        
%         \noindent (1) [SeriesDecomp] $\rightarrow$ $\text{Trend}^{(l)}$, $\text{Seasonal}^{(l)}$ \\
        
%         \qquad where \vspace{-30pt} \begin{flalign*}
%             & \hspace{70pt} \text{Trend}^{(l)} = \text{SMA}(\text{H}^{(l-1)}) && \\ 
%             & \hspace{70pt} \text{Seasonal}^{(l)} = \text{H}^{(l-1)} - \text{Trend}^{(l)} 
%         \end{flalign*}

%         \noindent (2) [selft-attention mechanism] on $\text{Seasonal}^{(l)}$

%         \qquad $\rightarrow$ $\text{Seasonal}'^{(l)}$ (+ residual connection, normalization, etc.) \\ 

%         \noindent (3) [Feed-Forward] on $\text{Seasonal}'^{(l)} \rightarrow \widetilde{\text{Seasonal}}^{(l)}$ \\
        
%         \noindent (4) Re-compose $\text{H}^{(l)} = \widetilde{\text{Seasonal}}^{(l)} + \text{Trend}^{(l)}$
%     \end{minipage}
% \end{center}
