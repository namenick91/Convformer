\section{Обозначения и предварительные сведения}

\label{sec:problem-setting}

Приведём формальное определение задачи долгосрочного прогнозирования временных рядов (LSTF). 
Рассматривается дискретный многомерный временной ряд
$
\{\mathbf{x}_t\}_{t=1}^T, \hspace{3pt} \mathbf{x}_t \in \mathbb{R}^{C_{\text{in}}}.
$
Пусть \(\mathbf{y}_t \in \mathbb{R}^{C_{\text{out}}}\) обозначает вектор таргетных компонент \(\mathbf{x}_t\),
причём \(C_{\text{out}} \le C_{\text{in}}\).
Для каждого момента времени \(t\), удовлетворяющего \(L_x \le t \le T - L_y\), определим входное окно наблюдений
$
\mathcal{X}_{t}
= \left\{ \mathbf{x}_{t-L_x+1}, \ldots, \mathbf{x}_{t} \right\},
$
и соответствующую ему последовательность будущих таргетных значений
$
\mathcal{Y}_{t}
= \left\{ \mathbf{y}_{t+1}, \ldots, \mathbf{y}_{t+L_y} \right\}.
$
Здесь и в дальнейшем: \(t\) -- глобальный индекс времени, \(L_x, L_y\) 
-- длины входной и выходной последовательностей, 
\(C_{\text{in}}, C_{\text{out}}\) -- число входных и прогнозируемых признаков (измеряемых величин) 
в каждый момент времени соответственно.
Дальнейшие рассуждения одинаково применимы к многомерному и одномерному случаям; различие заключается лишь в выборе \(C_{\text{out}}\) и в том, 
какие компоненты \(\mathbf{x}_t\) считаются таргетами.

Задача LSTF формулируется как задача обучения параметризованного отображения
$
f_\theta: \mathbb{R}^{L_x \times C_{\text{in}}} \rightarrow \mathbb{R}^{L_y \times C_{\text{out}}},
$
которое по входному окну наблюдений \(\mathcal{X}_{t}\) восстанавливает
соответствующую ему последовательность будущих значений \(\mathcal{Y}_{t}\):\quad
\begin{equation*}
\hat{\mathcal{Y}}_{t}
= f_\theta\left( \mathcal{X}_{t} \right)
\approx \mathcal{Y}_{t}.
\end{equation*}
Задача LSTF предполагает предсказание далёкого будущего, то есть больших значений \(L_y\); при этом размерность признаков не ограничивается одномерным случаем.
Пусть обучающая выборка состоит из набора окон
$
\mathcal{D} = \left\{ \left(\mathcal{X}_{t}, \mathcal{Y}_{t}\right) \right\}_{t \in \mathcal{T}},
$
где \(\mathcal{T}\) -- множество индексов времени, 
для которых формируются пары \((\mathcal{X}_t, \mathcal{Y}_t)\).
Обучение модели \(f_\theta\) проводится в постановке контролируемого обучения 
путём минимизации среднеквадратичной функции потерь:
\[
\mathcal{L}(\theta) 
= \frac{1}{|\mathcal{T}|}
\sum_{t \in \mathcal{T}}
\frac{1}{L_y \cdot C_{\text{out}}}
\left\|
f_\theta \left( \mathcal{X}_{t} \right)
- \mathcal{Y}_{t}
\right\|_F^2,
\]
где \(\|\cdot\|_F\) обозначает норму Фробениуса.
В дальнейшем под \(f_\theta\) рассматриваются как базовые трансформерные архитектуры 
Informer, Performer и Autoformer, так и предлагаемые в данной работе модификации. 
Все модели обучаются в единой постановке, отличаясь лишь внутренней реализацией 
отображения \(f_\theta\) при фиксированных 
\(L_x, L_y, C_{\text{in}}, C_{\text{out}}\).

% \subsection{Базовые компоненты архитектуры}

% Self-Attention
\subsection{Классический механизм внимания}
\label{sec:attention}
Пусть $\mathbf{Q} \in \mathbb{R}^{L_Q \times d_k}$,
$\mathbf{K} \in \mathbb{R}^{L_K \times d_k}$,
$\mathbf{V} \in \mathbb{R}^{L_K \times d_v}$ --
промежуточные представления входных данных, строки которых
можно интерпретировать как запросы, ключи и значения непрерывной
словарной структуры данных соответственно~\cite{transformer}, 
где $L_Q$ и $L_K$ обозначают длину последовательностей запросов и 
ключей/значений соответственно,
$d_k$ -- размерность запросов и ключей, 
а $d_v$ -- размерность значений.
Двунаправленное (bidirectional, или неориентированное,
non-directional~\cite{BERT}) внимание на основе скалярного
произведения имеет вид:
\begin{equation}
    \label{eq:attention}
    \mathrm{Att}_{\leftrightarrow}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) =
    \mathrm{softmax}\!\left( \frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}} \right)
    \mathbf{V}
    \in \mathbb{R}^{L_Q \times d_v},
\end{equation}
где $\mathrm{softmax}(\cdot)$ применяется построчно.

% Эквивалентно, выражение~\eqref{eq:attention} можно переписать в ядерной форме:
% \begin{equation*}
%     \mathrm{Att}_{\leftrightarrow}(\mathbf{q}_i, \mathbf{K}, \mathbf{V}) = 
%     \sum_j \frac{k(\mathbf{q}_i, \mathbf{k}_j)}{\sum_{l} k(\mathbf{q}_i, \mathbf{k}_l)} 
%     \,\mathbf{v}_j,
% \end{equation*}
% где $\mathbf{q}_i$ -- $i$-я строка матрицы $\mathbf{Q}$,
% $\mathbf{k}_j$ и $\mathbf{v}_j$ -- $j$-е строки матриц $\mathbf{K}$ и $\mathbf{V}$ соответственно, а 
% $k(\mathbf{q}_i, \mathbf{k}_j) \coloneq \mathrm{exp}(\mathbf{q}_i \mathbf{k}_j^\top / \sqrt{d_k})$ -- 
% экспоненциальное ядро.

Временная и пространственная сложность вычисления~\eqref{eq:attention}
равны $O(L_Q L_K (d_k + d_v))$ и
$O(L_Q L_K + L_Q d_k + L_K d_k + L_K d_v)$ соответственно
(при $L_Q = L_K = L$ и $d_v = d_k$ получаем $O(L^2 d_k)$ по времени
и $O(L^2 + Ld_k)$ по памяти). Поэтому механизм внимания на основе
скалярного произведения~\eqref{eq:attention} в принципе
несовместим с end-to-end-обработкой длинных
последовательностей. Двунаправленное внимание используется в
self-attention энкодера и в cross-attention энкодер-декодера
в архитектурах Seq2Seq.

% \rule{\linewidth}{0.1mm}

Другой важный тип внимания -- однонаправленное (unidirectional)
внимание:
\begin{equation*}
    \mathrm{Att}_{\rightarrow}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
    = \mathrm{softmax}\!\left(
        \frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}} + \mathbf{M}
    \right)\mathbf{V} \in \mathbb{R}^{L_Q \times d_v},
\end{equation*}
где $\mathbf{M} \in \mathbb{R}^{L_Q \times L_K}$ -- маска, элементы
которой задают, какие логиты участвуют в нормализации
(элементы, равные $-\infty$, полностью подавляют соответствующие
ключи для данного запроса). 

Однонаправленное внимание используется в self-attention декодера
в архитектурах Seq2Seq. В частности, в случае self-attention декодера,
когда $L_Q = L_K = L$, обычно используют нижнетреугольную каузальную
маску, задаваемую правилом
$M_{ij} = 0$ при $j \le i$ и $M_{ij} = -\infty$ при $j > i$.

На практике в трансформерных архитектурах используется стандартный
multi-head attention с $h$ головами~\cite{transformer}. 
В настоящей работе мы также используем multi-head attention; 
для простоты дальнейшего изложения ниже приводим запись для одной головы, 
а multi-head вариант получается параллельным применением механизма по головам 
с последующей конкатенацией результатов по признаковому измерению.

\paragraph{Механизм ProbSparse self-attention}
В Informer~\cite{informer} предлагается механизм ProbSparse self-attention, 
направленный на снижение квадратичной сложности стандартного внимания до 
логарифмического. 
Основная идея заключается в том, что для большинства запросов вклад 
большинства ключей в распределение attention оказывается пренебрежимо 
малым. В ProbSparse вводится статистика «важности» запроса, оценивающая 
разброс логитов по ключам, и для вычисления attention явно используются 
только запросы с наибольшей важностью. Это позволяет уменьшить вычислительные затраты по 
времени и памяти при сохранении основных пиков распределения внимания, 
однако вносит смещение за счёт явной выборки ключей и чувствительности 
к структуре данных.

\subsection{Ядерное внимание и random features}

Выражение~\eqref{eq:attention} можно эквивалентно переписать в 
ядерной форме:
\begin{equation}
    \label{eq:kernel_attention}
    \mathrm{Att}_{\leftrightarrow}(\mathbf{q}_i, \mathbf{K}, \mathbf{V}) = 
    \sum_{j=1}^{L_K} \cfrac{k(\mathbf{q}_i, \mathbf{k}_j)}{\sum_{l=1}^{L_K} k(\mathbf{q}_i, \mathbf{k}_l)} 
    \,\mathbf{v}_j,
\end{equation}
где $\mathbf{q}_i \in \mathbb{R}^{d_k}$ -- $i$-я строка матрицы $\mathbf{Q}$,
$\mathbf{k}_j \in \mathbb{R}^{d_k}$ и $\mathbf{v}_j \in \mathbb{R}^{d_v}$ -- $j$-е строки матриц $\mathbf{K}$ и $\mathbf{V}$ соответственно, а 
$k(\mathbf{q}_i, \mathbf{k}_j) \coloneq \mathrm{exp}(\mathbf{q}_i \mathbf{k}_j^\top / \sqrt{d_k})$ 
задаёт экспоненциальное (softmax) ядро.

Следуя Performer~\cite{performer}, без ограничения общности опустим 
$\sqrt{d_k}$-нормировку, поскольку можем 
эквивалентно перенормировать входные ключи и запросы, и далее будем работать с 
softmax-ядром: \,
% \begin{equation*}
%     \mathrm{SM}(\mathbf{x}, \mathbf{y}) \coloneq \exp(\mathbf{x}^\top \mathbf{y}).
% \end{equation*}
$
    \mathrm{SM}(\mathbf{x}, \mathbf{y}) \coloneq \exp(\mathbf{x}^\top \mathbf{y}).
$
Следуя подходу случайных признаков (random features)~\cite{RandomFeatures}, 
предположим, что для ядра $k$ существует случайное скалярное отображение вида:
\begin{equation*}
    k(\mathbf{x}, \mathbf{y}) = 
    \mathbb{E}_\omega [\varphi_\omega(\mathbf{x}) \varphi_\omega(\mathbf{y})],
    \qquad \varphi_\omega: \mathbb{R}^{d_k} \rightarrow \mathbb{R},
\end{equation*}
% так что при фиксированном наборе случайных параметров $\omega_1, \dots, \omega_m \in \mathbb{R}^{d_k}$ 
% можно построить эмпирическую карту признаков $\phi$, для которой:
% \begin{equation*}
%     k(\mathbf{x}, \mathbf{y}) \approx \phi(\mathbf{x})^\top \phi(\mathbf{y}),
% \end{equation*}
% где $r$ -- размерность карты признаков. Тогда мы можем аппроксимировать ядро в (\ref{eq:kernel_attention}) как:
% \begin{equation*}
%     k(\mathbf{q}_i, \mathbf{k}_j) \approx \phi(\mathbf{q}_i)^\top \phi(\mathbf{k}_j).
% \end{equation*}
так что при фиксированном наборе случайных параметров $\omega_1, \dots, \omega_m \in \mathbb{R}^{d_k}$ 
можно построить эмпирическую карту признаков 
$\phi(\mathbf{x}) = \tfrac{1}{\sqrt{m}}(\varphi_{\omega_1}(\mathbf{x}), \dots, \varphi_{\omega_m}(\mathbf{x})) \in \mathbb{R}^m$, 
для которой: \,
$
    k(\mathbf{x}, \mathbf{y}) \approx \phi(\mathbf{x})^\top \phi(\mathbf{y}).
$
% где $r$ -- размерность карты признаков. 
Тогда мы можем аппроксимировать ядро в (\ref{eq:kernel_attention}) как: \,
$
    k(\mathbf{q}_i, \mathbf{k}_j) \approx \phi(\mathbf{q}_i)^\top \phi(\mathbf{k}_j).
$
Подставляя эту аппроксимацию в~\eqref{eq:kernel_attention},
получаем линейную по длинам последовательностей $L_Q, L_K$ схему вычисления внимания:

\begin{equation}
    \label{eq:kernel-attention-approx}
    \mathrm{Att}_{\leftrightarrow}(\mathbf{q}_i, \mathbf{K}, \mathbf{V}) \approx 
    \frac{
      \phi(\mathbf{q}_i)^\top \left(\sum_{j=1}^{L_K} \phi(\mathbf{k}_j)\mathbf{v}_j^\top\right)
    }{
      \phi(\mathbf{q}_i)^\top \left(\sum_{l=1}^{L_K} \phi(\mathbf{k}_l)\right)
    }.
\end{equation}
Это позволяет обходиться без явного построения матрицы 
$\mathcal{K} \in \mathbb{R}^{L_Q \times L_K}, \quad
\mathcal{K}_{ij} = k(\mathbf{q}_i, \mathbf{k}_j)$.
При фиксированной размерности карты признаков $r$ вычислительная сложность 
становится линейной по $L$:\quad $O((L_Q + L_K) r (d_k + d_v))$
вместо квадратичной $O(L_Q L_K (d_k + d_v))$ у классического внимания. 
% ($O(L r d_k)$ против $O(L^2 d_k)$ если положить $L_Q = L_K = L$ и $d_v = d_k$).

Как показано в~\cite{performer,RandomFeatures}, широкий класс ядер можно реализовать 
через карты вида:
\begin{equation*}
\phi(\mathbf{x}) = \frac{h(\mathbf{x})}{\sqrt{m}}
\bigl( f_1(\omega_1^\top \mathbf{x}), \ldots, f_1(\omega_m^\top \mathbf{x}), 
       \ldots, 
       f_l(\omega_1^\top \mathbf{x}), \ldots, f_l(\omega_m^\top \mathbf{x}) \bigr)
\in\mathbb{R}^{r}, 
\end{equation*}
где $f_1,\dots,f_l : \mathbb{R} \to \mathbb{R}$, $h : \mathbb{R}^{d_k} \to \mathbb{R}$,
$\omega_1,\dots,\omega_m \stackrel{\mathrm{iid}}{\sim} \mathcal{D}$, для некоторого распределения 
$\mathcal{D} \in \mathcal{P}(\mathbb{R}^{d_k})$, и $r = l m$.

В частности, для несмещённой аппроксимации ядра $\mathrm{SM}(\mathbf{x}, \mathbf{y})$ 
можно использовать тригонометрическую карту случайных признаков \cite{HybridRandomFeatures, performer}: \,
% \begin{equation}
%     \label{eq:random-feature-map}
%     \phi(\mathbf{x}) = \frac{\exp\!\bigl(\|\mathbf{x}\|^2 / 2\bigr)}{\sqrt{m}} 
%     \bigl(\sin(\omega_1^\top \mathbf{x}), \dots, \sin(\omega_m^\top \mathbf{x}), 
%           \cos(\omega_1^\top \mathbf{x}), \dots, \cos(\omega_m^\top \mathbf{x})\bigr),
% \end{equation}
% где $\omega_1,\dots,\omega_m \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(\mathbf{0}, \mathbf{I}_{d_k})$ и $r = 2m$.
$h(\mathbf{x})=\exp(\tfrac{||\mathbf{x}||^2}{2}), l=2, f_1=\sin, f_2=\cos, 
\omega_i \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(\mathbf{0}, \mathbf{I}_{d_k})$. 
Назовем ее $\hat{\mathrm{SM}}^{\mathrm{trig}}_m$.

\paragraph{FAVOR+ как схема аппроксимации softmax-внимания} 
% Positive random features
% Orthogonal random features

В работе~\cite{performer} показано, что аппроксимация вида 
$\hat{\mathrm{SM}}^{\mathrm{trig}}_m$ из-за наличия потенциально отрицательных компонентов 
приводит к нестабильному поведению модели. Авторы предлагают более устойчивый
механизм аппроксимации softmax-ядра через карту положительных случайных
признаков (positive random feature map), снижающий дисперсию аппроксимации~\cite{performer}: \, 
% $h(\mathbf{x})=\tfrac{1}{\sqrt{2}}\exp(-\tfrac{||\mathbf{x}||^2}{2}), l=2, f_1(u)=\exp(u), f_2(u)=\exp(-u), 
% \mathcal{D}=\mathcal{N}(\mathbf{0},\mathbf{I}_{d_k})$. 
\begin{equation}
    \label{eq:PRF}
    \phi(\mathbf{x}) = 
        \cfrac{\exp({-\tfrac{||\mathbf{x}||^2}{2}})}{\sqrt{2m}}(
            e^{\omega_1^{\top}\mathbf{x}}, \dots, e^{\omega_m^{\top}\mathbf{x}},
            e^{-\omega_1^{\top}\mathbf{x}}, \dots, e^{-\omega_m^{\top}\mathbf{x}}
        ), \quad \omega_i \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(\mathbf{0}, \mathbf{I}_{d_k}).
\end{equation}

Дополнительно используется ортогонализация случайных признаков
(orthogonal random features). При изотропном распределении 
$\mathcal{D}$ (на практике чаще всего берут 
$\mathcal{N}(\mathbf{0}, \mathbf{I}_{d_k})$) ортогональные векторы
$\omega_i$ можно получать с помощью стандартной процедуры
ортогонализации Грама-Шмидта, сохраняя несмещённость оценок и ещё сильнее снижая
дисперсию аппроксимации softmax-ядра.

Подставляя полученное отображение в общую схему 
линейного внимания~\eqref{eq:kernel-attention-approx}, получаем механизм FAVOR+ 
(Fast Attention Via positive Orthogonal Random features)
для softmax-внимания, обладающий линейной по длине последовательности 
сложностью и теоретическими гарантиями на точность аппроксимации матрицы внимания.

% Positional / Temporal Embedding
\subsection{Представление входных данных}
\label{sec:input-representation}

По аналогии с Informer~\cite{informer} используем единое представление входного окна, 
объединяющее численные значения ряда, локальный позиционный контекст и глобальные календарные метки. 
Пусть 
\(\mathcal{X}_t \in \mathbb{R}^{L_x \times C_{\mathrm{in}}}\) -- окно наблюдений, а 
\(\mathcal{Z}_t \in \mathbb{Z}^{L_x \times p}\) -- матрица дискретных временных меток для каждого 
момента времени (месяц, день, день недели, час, при необходимости -- минута). Размерность скрытого 
пространства модели обозначим через \(d_{\mathrm{model}}\). 
Тогда эмбеддинговое представление 
\(\mathcal{H}_{t,0} \in \mathbb{R}^{L_x \times d_{\mathrm{model}}}\)
(блок ConvEmbedding, см. рис. \ref{fig:ConvformerScheme})
строится как сумма трёх слагаемых:
\begin{equation*}
    (\mathcal{X}_t, \mathcal{Z}_t)
    \;\mapsto\;
    \mathcal{H}_{t,0}
    = \mathrm{CS}(\mathcal{X}_t) + \mathrm{PE} + \mathrm{TE}(\mathcal{Z}_t),
\end{equation*}
где 
\(\mathrm{CS} = \mathrm{ConvStem}: \mathbb{R}^{L_x \times C_{\mathrm{in}}} \rightarrow \mathbb{R}^{L_x \times d_{\mathrm{model}}}\) 
    -- сверточная проекция значений (value embedding) (см. раздел \ref{sec:convstem}); 
\(\mathrm{PE} \in \mathbb{R}^{L_x \times d_{\mathrm{model}}}\) 
    -- матрица позиционного кодирования; 
\(\mathrm{TE}: \mathbb{Z}^{L_x \times p} \rightarrow \mathbb{R}^{L_x \times d_{\mathrm{model}}}\) 
    -- темпоральное кодирование.

\paragraph{Позиционное кодирование}
Используем синусоидальное позиционное кодирование, совпадающее с 
классическим Transformer~\cite{transformer}. 
Для позиции \(\text{pos} \in \{1,\ldots,L_x\}\) 
и индекса \(i \in \{0,\ldots,\lfloor d_{\mathrm{model}}/2 \rfloor - 1\}\) определим:
\[
\mathrm{PE}(\text{pos}, 2i)   = 
\sin\!\left(\cfrac{\text{pos}}{10000^{\,2i/d_{\mathrm{model}}}}\right), 
\qquad
\mathrm{PE}(\text{pos}, 2i+1) = 
\cos\!\left(\cfrac{\text{pos}}{10000^{\,2i/d_{\mathrm{model}}}}\right).
\]
Матрица \(\mathrm{PE} \in \mathbb{R}^{L_x \times d_{\mathrm{model}}}\) фиксируется заранее и не обучается.

\paragraph{Темпоральное кодирование}
Для каждого временного шага $i = 1,\ldots,L_x$ храним набор из $p$
дискретных календарных признаков: месяц, день, день недели, час и,
при необходимости, минуту. Сведём их в матрицу
$\mathcal{Z}_t \in \mathbb{Z}^{L_x \times p}$, где
$\mathcal{Z}_t[i,k] \in \{0,\ldots,S_k-1\}$ -- индекс категории
$k$-го календарного признака в момент времени, соответствующий позиции $i$,
а $S_k$ -- размер словаря для $k$-го признака.
Каждому типу календарной метки $k$ ставится в соответствие обучаемая
матрица эмбеддингов
$
E^{(k)} \in \mathbb{R}^{S_k \times d_{\mathrm{model}}},
$
которую удобно интерпретировать как отображение
$E^{(k)}:\{0,\ldots,S_k-1\}\rightarrow\mathbb{R}^{d_{\mathrm{model}}}$:
по индексу $\mathcal{Z}_t[i,k]$ возвращается вектор размерности
$d_{\mathrm{model}}$.

Отображение темпорального кодирования
$\mathrm{TE}: \mathbb{Z}^{L_x \times p} \to
\mathbb{R}^{L_x \times d_{\mathrm{model}}}$ определим покомпонентно:
\[
\mathrm{TE}(\mathcal{Z}_t)[i,:]
= \sum_{k=1}^{p} E^{(k)}\bigl[\mathcal{Z}_t[i,k],:\bigr],
\qquad i = 1,\ldots,L_x.
\]
Таким образом, темпоральный вектор для каждого шага получается
суммой эмбеддингов всех доступных календарных признаков
(месяц, день, день недели, час, минута).

% \paragraph{Темпоральное кодирование}
% Для каждого временного шага $i = 1,\ldots,L_x$ храним набор из $p$
% дискретных календарных признаков: месяц, день, день недели, час и,
% при необходимости, минуту. Сведём их в матрицу
% $\mathcal{Z}_t \in \mathbb{Z}^{L_x \times p}$, где
% $\mathcal{Z}_t[i,k]$ обозначает индекс категории $k$-го календарного
% признака в момент времени, соответствующий позиции $i$.
% Для каждого признака $k$ задаётся размер словаря $S_k$ 
% (например, $S_{\text{month}}=13$, $S_{\text{day}}=32$, 
% $S_{\text{weekday}}=7$), и предполагается, что
% \[
% \mathcal{Z}_t[i,k] \in \{0,1,\ldots,S_k-1\}
% \quad\text{для всех } i,k.
% \]

% Каждому типу календарной метки $k$ ставится в соответствие обучаемая
% матрица эмбеддингов
% \[
% E^{(k)} \in \mathbb{R}^{S_k \times d_{\mathrm{model}}},
% \]
% которую удобно интерпретировать как отображение
% $E^{(k)}:\{0,\ldots,S_k-1\}\rightarrow\mathbb{R}^{d_{\mathrm{model}}}$:
% по индексу $\mathcal{Z}_t[i,k]$ возвращается вектор размерности
% $d_{\mathrm{model}}$.

% Отображение темпорального кодирования
% $\mathrm{TE}: \mathbb{Z}^{L_x \times p} \to
% \mathbb{R}^{L_x \times d_{\mathrm{model}}}$ определим покомпонентно:
% \[
% \mathrm{TE}(\mathcal{Z}_t)[i,:]
% = \sum_{k=1}^{p} E^{(k)}\bigl[\mathcal{Z}_t[i,k],:\bigr],
% \qquad i = 1,\ldots,L_x.
% \]
% Таким образом, темпоральный вектор для каждого шага получается
% суммой эмбеддингов всех доступных календарных признаков
% (месяц, день, день недели, час, минута).

% \rule{\linewidth}{0.1mm}

% В совокупности блок ConvEmbedding реализует отображение
% \[
% (\mathcal{X}_t, \mathcal{Z}_t)
% \;\mapsto\;
% \mathcal{H}_{t,0}
% = \mathrm{ConvStem}(\mathcal{X}_t) + \mathrm{PE} + \mathbf{T}_t,
% \]
% которое далее используется в качестве входа энкодера и декодера.
