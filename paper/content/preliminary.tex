\section{Обозначения и предварительные сведения}

Приведём формальное определение задачи долгосрочного прогнозирования временных рядов (LSTF). 
Рассматривается дискретный многомерный временной ряд
$
\{\mathbf{x}_t\}_{t=1}^T, \hspace{3pt} \mathbf{x}_t \in \mathbb{R}^{C_{\text{in}}}.
$
Пусть \(\mathbf{y}_t \in \mathbb{R}^{C_{\text{out}}}\) обозначает вектор таргетных компонент \(\mathbf{x}_t\),
причём \(C_{\text{out}} \le C_{\text{in}}\).
Для каждого момента времени \(t\), удовлетворяющего \(L_x \le t \le T - L_y\), определим входное окно наблюдений
$
\mathcal{X}_{t}
= \left\{ \mathbf{x}_{t-L_x+1}, \ldots, \mathbf{x}_{t} \right\},
$
и соответствующую ему последовательность будущих таргетных значений
$
\mathcal{Y}_{t}
= \left\{ \mathbf{y}_{t+1}, \ldots, \mathbf{y}_{t+L_y} \right\}.
$
Здесь и в дальнейшем: \(t\) - глобальный индекс времени, \(L_x, L_y\) - длины входной и выходной последовательностей, 
\(C_{\text{in}}, C_{\text{out}}\) - число входных и прогнозируемых признаков (измеряемых величин) 
в каждый момент времени соответственно.
Дальнейшие рассуждения одинаково применимы к многомерному и одномерному случаям; различие заключается лишь в выборе \(C_{\text{out}}\) и в том, 
какие компоненты \(\mathbf{x}_t\) считаются таргетами.

Задача LSTF формулируется как задача обучения параметризованного отображения
$
f_\theta: \mathbb{R}^{L_x \times C_{\text{in}}} \rightarrow \mathbb{R}^{L_y \times C_{\text{out}}},
$
которое по входному окну наблюдений \(\mathcal{X}_{t}\) восстанавливает
соответствующую ему последовательность будущих значений \(\mathcal{Y}_{t}\):\quad
\begin{equation*}
\hat{\mathcal{Y}}_{t}
= f_\theta\left( \mathcal{X}_{t} \right)
\approx \mathcal{Y}_{t}.
\end{equation*}
Задача LSTF предполагает предсказание далёкого будущего, то есть больших значений \(L_y\); при этом размерность признаков не ограничивается одномерным случаем.
Пусть обучающая выборка состоит из набора окон
$
\mathcal{D} = \left\{ \left(\mathcal{X}_{t}, \mathcal{Y}_{t}\right) \right\}_{t \in \mathcal{T}},
$
где \(\mathcal{T}\) - множество индексов времени, для которых формируются пары \((\mathcal{X}_t, \mathcal{Y}_t)\).
Обучение модели \(f_\theta\) проводится в постановке контролируемого обучения путём минимизации среднеквадратичной функции потерь:
\[
\mathcal{L}(\theta) 
= \frac{1}{|\mathcal{T}|}
\sum_{t \in \mathcal{T}}
\frac{1}{L_y \cdot C_{\text{out}}}
\left\|
f_\theta \left( \mathcal{X}_{t} \right)
- \mathcal{Y}_{t}
\right\|_F^2,
\]
где \(\|\cdot\|_F\) обозначает норму Фробениуса.
В дальнейшем под \(f_\theta\) рассматриваются как базовые трансформерные архитектуры Informer, Performer и Autoformer, так и предлагаемые в данной работе модификации. 
Все модели обучаются в единой постановке, отличаясь лишь внутренней реализацией отображения \(f_\theta\) при фиксированных 
\(L_x, L_y, C_{\text{in}}, C_{\text{out}}\).

\subsection{Базовые компоненты архитектуры}

% Self-Attention
\paragraph{Классический механизм внимания.}
{\color{red} дописать multi-head attention?}
Пусть $\mathbf{Q} \in \mathbb{R}^{L_Q \times d_k}$,
$\mathbf{K} \in \mathbb{R}^{L_K \times d_k}$,
$\mathbf{V} \in \mathbb{R}^{L_K \times d_v}$ --
промежуточные представления входных данных, строки которых
можно интерпретировать как запросы, ключи и значения непрерывной
словарной структуры данных соответственно~\cite{transformer}, 
где $L_Q$ и $L_K$ обозначают длину последовательностей запросов и 
ключей/значений соответственно,
$d_k$ -- размерность запросов и ключей, 
а $d_v$ -- размерность значений.
Двунаправленное (bidirectional, или неориентированное
(non-directional)~\cite{BERT}) внимание на основе скалярного
произведения имеет вид:
\begin{equation}
    \label{eq:attention}
    \mathrm{Att}_{\leftrightarrow}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) =
    \mathrm{softmax}\!\left( \frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}} \right)
    \mathbf{V}
    \in \mathbb{R}^{L_Q \times d_v},
\end{equation}
где $\mathrm{softmax}(\cdot)$ применяется построчно.

Временная и пространственная сложность вычисления~\eqref{eq:attention}
равны $O(L_Q L_K (d_k + d_v))$ и
$O(L_Q L_K + L_Q d_k + L_K d_k + L_K d_v)$ соответственно
(при $L_Q = L_K = L$ и $d_v = d_k$ получаем $O(L^2 d_k)$ по времени
и $O(L^2 + Ld_k)$ по памяти). Поэтому механизм внимания на основе
скалярного произведения~\eqref{eq:attention} в принципе
несовместим с end-to-end-обработкой длинных
последовательностей. Двунаправленное внимание используется в
self-attention энкодера и в cross-attention энкодер–декодера
в архитектурах Seq2Seq.

Другой важный тип внимания -- однонаправленное (unidirectional)
внимание:
\begin{equation*}
    \mathrm{Att}_{\rightarrow}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
    = \mathrm{softmax}\!\left(
        \frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}} + \mathbf{M}
    \right)\mathbf{V} \in \mathbb{R}^{L_Q \times d_v},
\end{equation*}
где $\mathbf{M} \in \mathbb{R}^{L_Q \times L_K}$ -- маска, элементы
которой задают, какие логиты участвуют в нормализации
(элементы, равные $-\infty$, полностью подавляют соответствующие
ключи для данного запроса). 

Однонаправленное внимание используется в self-attention декодера
в архитектурах Seq2Seq. В частности, в случае self-attention декодера,
когда $L_Q = L_K = L$, обычно используют нижнетреугольную каузальную
маску, задаваемую правилом
$M_{ij} = 0$ при $j \le i$ и $M_{ij} = -\infty$ при $j > i$.

% FAVOR+
\paragraph{Аппроксимация softmax-ядер для механизма внимания.}
{\color{red} TODO}

% Series Decomposition
\paragraph{Декомпозиция ряда.}
Следуя Autoformer~\cite{autoformer}, опишем механизм 
декомпозиции ряда (SeriesDecomp) -- внутреннюю операцию, 
которая постепенно извлекает долгосрочную трендовую 
(трендово-циклическую) составляющую из предсказанных 
промежуточных скрытых представлений. Формально:
\begin{gather*}
    \mathcal{X}_t^{(\mathrm{tr})} = \mathrm{MA}_k(\mathcal{X}_t), \\ 
    \mathcal{X}_t^{(\mathrm{se})} = \mathcal{X}_t - \mathcal{X}_t^{(\mathrm{tr})},
\end{gather*}
где $\mathcal{X}_t^{(\mathrm{tr})}, \mathcal{X}_t^{(\mathrm{se})} \in \mathbb{R}^{L_x \times d_{\mathrm{model}}}$ -- 
трендово-циклическая и сезонная составляющие соответственно, 
$\mathrm{MA}_k(\cdot)$ -- оператор скользящего среднего 
(simple moving average) с окном длины $k$, единичным шагом 
и постоянным дополнением на границах. В дальнейшем будем 
использовать обозначение
$
(\mathcal{X}_t^{(\mathrm{tr})}, \mathcal{X}_t^{(\mathrm{se})}) 
= \mathrm{SeriesDecomp}(\mathcal{X}_t)
$
для краткой записи описанного выше блока.

% Positional / Temporal Embedding
\paragraph{Представление входных данных.}
{\color{red} TODO}

% \rule{\linewidth}{0.1mm}

% Self-Attention Distilling
\paragraph{Дистилляция внимания.}
{\color{red} TODO}
