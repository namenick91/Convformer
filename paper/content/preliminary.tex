\section{Обозначения и предварительные сведения}

Приведём формальное определение задачи долгосрочного прогнозирования временных рядов (LSTF). 
Рассматривается дискретный многомерный временной ряд
$
\{\mathbf{x}_t\}_{t=1}^T, \hspace{3pt} \mathbf{x}_t \in \mathbb{R}^{C_{\text{in}}}.
$
Пусть \(\mathbf{y}_t \in \mathbb{R}^{C_{\text{out}}}\) обозначает вектор таргетных компонент \(\mathbf{x}_t\),
причём \(C_{\text{out}} \le C_{\text{in}}\).
Для каждого момента времени \(t\), удовлетворяющего \(L_x \le t \le T - L_y\), определим входное окно наблюдений
$
\mathcal{X}_{t}
= \left\{ \mathbf{x}_{t-L_x+1}, \ldots, \mathbf{x}_{t} \right\},
$
и соответствующую ему последовательность будущих таргетных значений
$
\mathcal{Y}_{t}
= \left\{ \mathbf{y}_{t+1}, \ldots, \mathbf{y}_{t+L_y} \right\}.
$
Здесь и в дальнейшем: \(t\) - глобальный индекс времени, \(L_x, L_y\) - длины входной и выходной последовательностей, 
\(C_{\text{in}}, C_{\text{out}}\) - число входных и прогнозируемых признаков (измеряемых величин) 
в каждый момент времени соответственно.
Дальнейшие рассуждения одинаково применимы к многомерному и одномерному случаям; различие заключается лишь в выборе \(C_{\text{out}}\) и в том, 
какие компоненты \(\mathbf{x}_t\) считаются таргетами.
Задача LSTF формулируется как задача обучения параметризованного отображения
$
f_\theta: \mathbb{R}^{L_x \times C_{\text{in}}} \rightarrow \mathbb{R}^{L_y \times C_{\text{out}}},
$
которое по входному окну наблюдений \(\mathcal{X}_{t}\) восстанавливает
соответствующую ему последовательность будущих значений \(\mathcal{Y}_{t}\):\quad
$
\hat{\mathcal{Y}}_{t}
= f_\theta\left( \mathcal{X}_{t} \right)
\approx \mathcal{Y}_{t}.
$
Задача LSTF предполагает предсказание далёкого будущего, то есть больших значений \(L_y\); при этом размерность признаков не ограничивается одномерным случаем.
Пусть обучающая выборка состоит из набора окон
$
\mathcal{D} = \left\{ \left(\mathcal{X}_{t}, \mathcal{Y}_{t}\right) \right\}_{t \in \mathcal{T}},
$
где \(\mathcal{T}\) - множество индексов времени, для которых формируются пары \((\mathcal{X}_t, \mathcal{Y}_t)\).
Обучение модели \(f_\theta\) проводится в постановке контролируемого обучения путём минимизации среднеквадратичной функции потерь:
\[
\mathcal{L}(\theta) 
= \frac{1}{|\mathcal{T}|}
\sum_{t \in \mathcal{T}}
\frac{1}{L_y \cdot C_{\text{out}}}
\left\|
f_\theta \left( \mathcal{X}_{t} \right)
- \mathcal{Y}_{t}
\right\|_F^2,
\]
где \(\|\cdot\|_F\) обозначает норму Фробениуса.
В дальнейшем под \(f_\theta\) рассматриваются как базовые трансформерные архитектуры Informer, Performer и Autoformer, так и предлагаемые в данной работе модификации. 
Все модели обучаются в единой постановке, отличаясь лишь внутренней реализацией отображения \(f_\theta\) при фиксированных 
\(L_x, L_y, C_{\text{in}}, C_{\text{out}}\).
