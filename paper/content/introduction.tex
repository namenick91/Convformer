\section{Введение}

Долгосрочное прогнозирование временных рядов является одной из ключевых задач
во многих областях - от энергетики и финансов до транспорта и здравоохранения.
Однако реальные многомерные ряды редко обладают простой структурой: среди прочего, 
они сочетают краткосрочные локальные паттерны, очень длинные и межрядовые
зависимости, а также выраженную нестационарность, включая сдвиги тренда,
мультисезонность и гетероскедастичность. Эти свойства затрудняют обучение
моделей и ухудшают их способность к экстраполяции на длительных горизонтах.

С момента публикации, архитектура Трансформера~\cite{transformer} завоевала
широкое признание. Однако у нее есть несколько серьезных проблем, 
которые усложняют работу с длинными временными последовательностями
(LSTF). Последующие исследования предложили различные методы решения 
данных и связанных с ними проблем (Informer~\cite{informer},
Autoformer~\cite{autoformer}, Performer~\cite{performer}). Тем не менее,
существующие архитектуры сталкиваются с рядом ограничений: необходимость
одновременного учёта локальных и глобальных закономерностей, высокая
чувствительность к нестационарности и ограниченная масштабируемость по длине
входной последовательности.

В данной работе мы предлагаем архитектуру, которая объединяет три
взаимодополняющих индуктивных смещения, специально ориентированных на решение
этих вызовов:  
\textbf{(i)} 
расширенный сверточный входной блок, способный эффективно кодировать 
краткосрочные локальные паттерны (скачки, ступени, импульсные всплески). 
Такой модуль выполняет роль фильтра низкого уровня и одновременно стабилизирует 
статистики входных данных, что повышает устойчивость модели к локальной нестационарности;
\textbf{(ii)} 
механизм внимания FAVOR+ (Performer~\cite{performer}), обеспечивающий линейные
$O(Lr)$ затраты времени и памяти; параметр ранга $r$ задаёт настраиваемый
компромисс между скоростью и точностью. 
Это позволяет обрабатывать очень длинные последовательности и улавливать отложенные 
зависимости без взрыва затрат, что критично для прогнозов на больших горизонтах. 
\textbf{(iii)} 
явная декомпозиция ряда в стиле Autoformer~\cite{autoformer} после каждого блока self-attention: 
низкочастотный тренд выводится через остаточные соединения, а очищенный от 
тренда стационаризированный остаток подаётся в механизм внимания. Такой приём 
снижает влияние сдвигов уровня и многосезонности, улучшая обобщающую способность модели. 

Такая композиция локальной фильтрации, глобального внимания и явной
декомпозиции улучшает точность прогнозирования на длинных горизонтах,
ускоряет и стабилизирует оптимизацию, а также снижает затраты памяти по
сравнению с базовыми трансформер-подходами. Основной вклад данной работы можно 
резюмировать следующим образом:
\begin{itemize}
    \item Вводим расширенный сверточный входной блок для кодирования краткосрочных
    мотивов и стабилизации статистик нестационарных входов.  

    \item Для моделирования глобальных связей применяем FAVOR+ внимание с линейными затратами 
    по времени и памяти, что обеспечивает 
    масштабируемое и не зависящее от распределения моделирование глобальных зависимостей;  

    \item Встраиваем Autoformer-подобную\cite{autoformer} декомпозицию после каждого блока 
    self-attention, которая пропускает низкочастотный тренд по остаточным соединениям и 
    передаёт более стационаризированный поток данных в механизм внимания.

    \item Эксперименты на стандартных бенчмарках
    демонстрируют устойчивое снижение ошибок на длительных горизонтах
    прогнозирования; абляционные исследования подтверждают вклад каждого модуля,
    а анализ масштабируемости показывает благоприятный баланс между точностью и
    вычислительными затратами.
\end{itemize}
