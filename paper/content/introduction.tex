\section{Введение}

Долгосрочное прогнозирование временных рядов является одной из ключевых задач
во многих областях -- от энергетики и финансов до транспорта и здравоохранения.
Однако реальные многомерные ряды редко обладают простой структурой: среди прочего, 
они сочетают краткосрочные локальные паттерны, очень длинные и межрядовые
зависимости, а также выраженную нестационарность, включая сдвиги тренда,
мультисезонность и гетероскедастичность. Эти свойства затрудняют обучение
моделей и ухудшают их способность к экстраполяции на длительных горизонтах;
классические статистические методы (ARIMA, VAR, экспоненциальное сглаживание и др.)
обычно эффективны лишь на умеренных горизонтах и почти стационарных данных.

С момента публикации архитектура Transformer~\cite{transformer} завоевала
широкое признание. Однако у нее есть несколько серьезных проблем, 
которые усложняют работу с длинными временными последовательностями
(LSTF). Последующие исследования предложили различные методы решения 
данных и связанных с ними проблем: 
Informer~\cite{informer}
вводит разреженное ProbSparse внимание и схему distilling для снижения квадратичных
затрат по длине последовательности; Autoformer~\cite{autoformer} дополняет
архитектуру явной декомпозицией ряда на тренд и сезонный остаток и модулем
AutoCorrelation для устойчивого прогнозирования на длинных горизонтах; Performer~\cite{performer}
предлагает FAVOR+ -- несмещённую аппроксимацию softmax-ядра с линейной по длине
последовательности сложностью. Тем не менее
существующие архитектуры сталкиваются с рядом ограничений: необходимость
одновременного учёта локальных и глобальных закономерностей, высокая
чувствительность к нестационарности и ограниченная масштабируемость по длине
входной последовательности.

% В данной работе мы предлагаем архитектуру, которая объединяет три
% взаимодополняющих индуктивных смещения, специально ориентированных на решение
% этих вызовов:  
% \textbf{(i)} 
% расширенный сверточный входной блок, способный эффективно кодировать 
% краткосрочные локальные паттерны (скачки, ступени, импульсные всплески). 
% Такой модуль выполняет роль фильтра низкого уровня и одновременно стабилизирует 
% статистики входных данных, что повышает устойчивость модели к локальной нестационарности;
% \textbf{(ii)} 
% механизм внимания FAVOR+ (Performer~\cite{performer}), обеспечивающий линейные
% $O(Lr)$ затраты времени и памяти; параметр ранга $r$ задаёт настраиваемый
% компромисс между скоростью и точностью. 
% Это позволяет обрабатывать очень длинные последовательности и улавливать отложенные 
% зависимости без взрыва затрат, что критично для прогнозов на больших горизонтах. 
% \textbf{(iii)} 
% явная декомпозиция ряда в стиле Autoformer~\cite{autoformer} после каждого блока self-attention: 
% низкочастотный тренд выводится через остаточные соединения, а очищенный от 
% тренда стационаризированный остаток подаётся в механизм внимания. Такой приём 
% снижает влияние сдвигов уровня и многосезонности, улучшая обобщающую способность модели. 

В данной работе мы предлагаем архитектуру Convformer, которая в единой схеме
объединяет три взаимодополняющих индуктивных смещения, специально ориентированных
на решение перечисленных проблем: сверточный входной блок ConvStem для выделения
локальных паттернов, линейное внимание FAVOR+ для масштабируемого моделирования
глобальных зависимостей и Autoformer-подобную декомпозицию ряда после каждого
блока self-attention.

% Такая композиция локальной фильтрации, глобального внимания и явной
% декомпозиции улучшает точность прогнозирования на длинных горизонтах,
% ускоряет и стабилизирует оптимизацию, а также снижает затраты памяти по
% сравнению с базовыми трансформер-подходами. 

Основной вклад данной работы можно 
резюмировать следующим образом:
\begin{itemize}
    \item Вводим расширенный сверточный входной блок для кодирования краткосрочных
    мотивов (скачки, ступени, импульсные всплески) и стабилизации статистик
    нестационарных входов.

    \item Для моделирования глобальных связей применяем механизм внимания FAVOR+ с 
    линейными затратами по времени и памяти, что обеспечивает 
    масштабируемое моделирование дальних зависимостей.  

    \item Встраиваем Autoformer-подобную~\cite{autoformer} декомпозицию после каждого блока 
    self-attention, которая пропускает низкочастотный тренд по остаточным соединениям и 
    передаёт более стационаризированный поток данных в механизм внимания.

    \item Эксперименты на стандартных бенчмарках
    демонстрируют снижение ошибок на длительных горизонтах
    прогнозирования; абляционные исследования подтверждают вклад каждого модуля,
    а сравнительный анализ показывает благоприятный баланс между точностью и
    вычислительными затратами.
\end{itemize}
